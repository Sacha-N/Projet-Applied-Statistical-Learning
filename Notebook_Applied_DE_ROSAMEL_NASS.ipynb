{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efb09aad",
   "metadata": {},
   "source": [
    "# Applied Statistical Learning - Prédiction de la durée de mise en chantier en France\n",
    "Dans le cadre de ce projet, nous avons exploité les données du fichier SITADEL, qui recense de manière exhaustive l'ensemble des permis de construire en France. Pour avoir un échantillon homogène, nous nous sommes concentrés sur la période 2015-2019 (i.e. avant les délais liés au Covid). Nous avons enrichi notre fichier à l'aide de deux sources externes: le fichier complet de l'Insee, agrégeant des données en open data pour l'ensemble des communes, ainsi que la grille de densité des communes de l'Insee. \n",
    "Pour faciliter la réplication de nos résultats, nous avons déposé nos données brutes sur une dropbox (1.5Go) : https://www.dropbox.com/scl/fo/b3dw5eht79785mrg7uueq/APQ7m21mQcsZzh7Uy7ASj6k?rlkey=rutboi86sxni30nqg6lzsycqh&st=n60gni1d&dl=0. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04dcffa",
   "metadata": {},
   "source": [
    "# 1. Mise en place des chemins\n",
    "Pour faire tourner l'ensemble du code de ce notebook, il est nécessaire de placer l'ensemble des fichiers de la dropbox dans un dossier data, puis d'adapter le chemin ci-dessous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8319cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# CHEMIN A ADAPTER\n",
    "chemin_data = Path(\"data\")   # mettre le chemin local vers le dossier data avec les fichiers de la dropbox\n",
    "\n",
    "# Suffixes fixes\n",
    "chemin_autorisation = chemin_data / \"Liste-des-autorisations-durbanisme-creant-des-logements.2025-10.csv\"\n",
    "chemin_grilles = chemin_data / \"grille_densite_7_niveaux_2019.xlsx\"\n",
    "chemin_dossier = chemin_data / \"dossier_complet.csv\"\n",
    "\n",
    "# installer pyarrow pour lire les fichiers parquet, et openpyxl pour lire les fichiers excel\n",
    "%pip install pyarrow fastparquet\n",
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1bf53f",
   "metadata": {},
   "source": [
    "# 2. Chargement des données\n",
    "Ce premier chunk permet de sélectionner nos variables d'intérêt, et d'observer notre base avant tout filtre. Le fichier contient initialement 1,8 million de permis de construire, couvrant souvent plusieurs logements. Plusieurs variables d'intérêt sont possibles : la durée avant l'obtention de l'autorisation, la durée entre l'obtention de l'autorisation et la mise en chantier, ou la durée du chantier. Afin d'éviter de nous restreindre à des chantiers terminés, nous étudierons la **durée de mise en chantier**. Après le retrait de durées absentes ou négatives, nous avons 1,7 million de permis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28025f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Chargement de Sitadel\n",
    "# Chemin : défini en amont, vérifier que le chunk a bien été executé. \n",
    "\n",
    "# 1.1 Exclusion ex ante des colonnes non pertinentes\n",
    "all_cols = pd.read_csv(chemin_autorisation, sep=\";\", skiprows=1, nrows=1).columns.tolist()\n",
    "\n",
    "vars_mai2022 = [ #On retire les colonnes qui n'existent qu'à partir de mai 2022 (car on veut un dataset homogène dans le temps)\n",
    "    \"AN_DEPOT\",  # \"DPC_PREM\", (theoriquement il faudrait la retirer, mais bon)\n",
    "    \"NATURE_PROJET_COMPLETEE\",\n",
    "    \"DESTINATION_PRINCIPALE\",\n",
    "    \"TYPE_PRINCIP_LOGTS_CREES\",\n",
    "    \"TYPE_TRANSFO_PRINCIPAL\",\n",
    "    \"TYPE_PRINCIP_LOCAUX_TRANSFORMES\",\n",
    "    \"I_PISCINE\",\n",
    "    \"I_GARAGE\",\n",
    "    \"I_VERANDA\",\n",
    "    \"I_ABRI_JARDIN\",\n",
    "    \"I_AUTRE_ANNEXE\",\n",
    "    \"RES_PERS_AGEES\",\n",
    "    \"RES_ETUDIANTS\",\n",
    "    \"RES_TOURISME\",\n",
    "    \"RES_HOTEL_SOCIALE\",\n",
    "    \"RES_SOCIALE\",\n",
    "    \"RES_HANDICAPES\",\n",
    "    \"RES_AUTRE\",\n",
    "    \"NB_LGT_INDIV_PURS\",\n",
    "    \"NB_LGT_INDIV_GROUPES\",\n",
    "    \"NB_LGT_RES\",\n",
    "    \"NB_LGT_COL_HORS_RES\",\n",
    "    \"SUgbr_HEB_TRANSFORMEE\",\n",
    "    \"SUgbr_BUR_TRANSFORMEE\",\n",
    "    \"SUgbr_COM_TRANSFORMEE\",\n",
    "    \"SUgbr_ART_TRANSFORMEE\",\n",
    "    \"SUgbr_IND_TRANSFORMEE\",\n",
    "    \"SUgbr_AGR_TRANSFORMEE\",\n",
    "    \"SURF_ENT_TRANSFORMEE\",\n",
    "    \"SURF_PUB_TRANSFORMEE\",\n",
    "]\n",
    "vars_non_pertinentes = [ # on retire les variables sans pouvoir prédictif (ex : SIREN du demandeur)\n",
    "    \"Num_DAU\",\n",
    "    \"SIREN_DEM\",\n",
    "    \"SIRET_DEM\",\n",
    "    \"DENOM_DEM\",\n",
    "    \"CODPOST_DEM\",\n",
    "    \"LOCALITE_DEM\",\n",
    "    \"ADR_NUM_TER\",\n",
    "    \"ADR_TYPEVOIE_TER\",\n",
    "    \"ADR_LIBVOIE_TER\",\n",
    "    \"ADR_LIEUDIT_TER\",\n",
    "    \"ADR_LOCALITE_TER\",\n",
    "    \"ADR_CODPOST_TER\",\n",
    "    \"SEC_CADASTRE1\",\n",
    "    \"NUM_CADASTRE1\",\n",
    "    \"SEC_CADASTRE2\",\n",
    "    \"NUM_CADASTRE2\",\n",
    "    \"SEC_CADASTRE3\",\n",
    "    \"NUM_CADASTRE3\",\n",
    "]\n",
    "\n",
    "cols_to_drop = set(vars_mai2022 + vars_non_pertinentes)\n",
    "use_cols = [c for c in all_cols if c not in cols_to_drop]\n",
    "\n",
    "# 1.2 Chargement avec les colonnes filtrées\n",
    "df = pd.read_csv(\n",
    "    chemin_autorisation,\n",
    "    sep=\";\",\n",
    "    encoding=\"utf-8\",\n",
    "    skiprows=1,\n",
    "    usecols=use_cols\n",
    ")\n",
    "\n",
    "# 3. Etudions nos dates\n",
    "date_cols = [\n",
    "    \"DATE_REELLE_AUTORISATION\",\n",
    "    \"DATE_REELLE_DOC\",\n",
    "    \"DPC_AUT\",\n",
    "    \"DATE_REELLE_DAACT\",\n",
    "    \"DPC_PREM\",\n",
    "]\n",
    "\n",
    "for col in date_cols:\n",
    "    if col in df.columns:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Type: {df[col].dtype}\")\n",
    "        print(\"  Sample values:\")\n",
    "        print(df[col].head(10).tolist())\n",
    "        print(f\"  Null count: {df[col].isna().sum()}\")\n",
    "    else:\n",
    "        print(f\"\\n{col}: NOT FOUND in dataframe\")\n",
    "\n",
    "\n",
    "## Puis, nettoyons le dataset\n",
    "def nettoyer_dataset(df: pd.DataFrame):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Conversion des dates en datetime\n",
    "    # 3 dates en format DD/MM/YYYY\n",
    "    dmY_cols = [\"DATE_REELLE_AUTORISATION\", \"DATE_REELLE_DOC\", \"DATE_REELLE_DAACT\"]\n",
    "    for col in dmY_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(\n",
    "                df[col], errors=\"coerce\", dayfirst=True\n",
    "            )\n",
    "\n",
    "    # 2 dates en format YYYY-MM\n",
    "    for col in [\"DPC_AUT\", \"DPC_PREM\"]:\n",
    "        if col in df.columns:\n",
    "            # on impose un format\n",
    "            df[col] = pd.to_datetime(\n",
    "                df[col].astype(str).str.strip(), errors=\"coerce\", format=\"%Y-%m\"\n",
    "            )\n",
    "\n",
    "    # On retire les dates fausses\n",
    "    min_date = pd.Timestamp(\"1900-01-01\") \n",
    "    max_date = pd.Timestamp(\"2025-12-31\")\n",
    "    for col in dmY_cols + [\"DPC_AUT\", \"DPC_PREM\"]:\n",
    "        if col in df.columns:\n",
    "            mask = (df[col] < min_date) | (df[col] > max_date)\n",
    "            df.loc[mask, col] = pd.NaT\n",
    "\n",
    "    # On construit trois variables cibles, même si in fine on n'utilisera que delai_ouverture_chantier\n",
    "    # Nous avons laissé notre code car nous avions essayé avec d'autres variables cibles, finalement moins faciles à prédire.\n",
    "    if \"DATE_REELLE_AUTORISATION\" in df.columns and \"DATE_REELLE_DOC\" in df.columns:\n",
    "        mask = df[\"DATE_REELLE_AUTORISATION\"].notna() & df[\"DATE_REELLE_DOC\"].notna()\n",
    "        df.loc[mask, \"delai_ouverture_chantier\"] = (\n",
    "            df.loc[mask, \"DATE_REELLE_DOC\"] - df.loc[mask, \"DATE_REELLE_AUTORISATION\"]\n",
    "        ).dt.days\n",
    "\n",
    "    if \"DATE_REELLE_DAACT\" in df.columns and \"DATE_REELLE_DOC\" in df.columns:\n",
    "        mask = df[\"DATE_REELLE_DAACT\"].notna() & df[\"DATE_REELLE_DOC\"].notna()\n",
    "        df.loc[mask, \"duree_travaux\"] = (\n",
    "            df.loc[mask, \"DATE_REELLE_DAACT\"] - df.loc[mask, \"DATE_REELLE_DOC\"]\n",
    "        ).dt.days\n",
    "\n",
    "    if \"DPC_AUT\" in df.columns and \"DPC_PREM\" in df.columns:\n",
    "        mask = df[\"DPC_PREM\"].notna() & df[\"DPC_AUT\"].notna()\n",
    "        df.loc[mask, \"duree_obtiention_autorisation\"] = (\n",
    "            df.loc[mask, \"DPC_AUT\"] - df.loc[mask, \"DPC_PREM\"]\n",
    "        ).dt.days\n",
    "\n",
    "    # On traite les variables cibles: conversion en numérique et suppression des valeurs négatives\n",
    "    duration_cols = [\n",
    "        \"delai_ouverture_chantier\",\n",
    "        \"duree_travaux\",\n",
    "        \"duree_obtiention_autorisation\",\n",
    "    ]\n",
    "    for col in duration_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "            df.loc[df[col] <= 0, col] = pd.NA\n",
    "\n",
    "    # Suppression des lignes sans cibles valides (NA, null, zero or negative)\n",
    "    existing_duration_cols = [c for c in duration_cols if c in df.columns]\n",
    "    df = df.dropna(subset=existing_duration_cols, how=\"all\")\n",
    "    \n",
    "    # On crée mois et année\n",
    "    df[\"annee_autorisation\"] = df[\"DATE_REELLE_AUTORISATION\"].dt.year\n",
    "    df[\"mois_autorisation\"] = df[\"DATE_REELLE_AUTORISATION\"].dt.month\n",
    "    return df\n",
    "\n",
    "\n",
    "df_clean = nettoyer_dataset(df)\n",
    "\n",
    "# On règle également les codes (dep, commune, région)\n",
    "colonnes_codes = [\n",
    "    \"DEP_CODE\",\n",
    "    \"COMM\",\n",
    "    \"CODGEO\",\n",
    "    \"REG_CODE\"\n",
    "]\n",
    "\n",
    "for col in colonnes_codes:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = df_clean[col].astype(\"string\")\n",
    "\n",
    "# On enregistre en parquet pour conserver les formats\n",
    "df_clean.to_parquet(\n",
    "    chemin_data / \"autorisations.parquet\",\n",
    "    engine=\"pyarrow\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "## 4. Comparaison\n",
    "# Avant nettoyage\n",
    "print(\"Df shape avant:\", df.shape)\n",
    "print(\"Nombre de lignes dans le df original:\", len(df))\n",
    "\n",
    "# Après nettoyage\n",
    "print(\"\\n Après nettoyages :\")\n",
    "print(\n",
    "    \"Lignes avec delai_ouverture_chantier non-NA:\",\n",
    "    df_clean[\"delai_ouverture_chantier\"].notna().sum(),\n",
    ")\n",
    "print(\"Lignes avec duree_travaux non-NA:\", df_clean[\"duree_travaux\"].notna().sum())\n",
    "print(\n",
    "    \"Lignes avec duree_obtiention_autorisation non-NA:\",\n",
    "    df_clean[\"duree_obtiention_autorisation\"].notna().sum(),\n",
    ")\n",
    "print(\n",
    "    \"Lignes avec duree_obtiention_autorisation non-0:\",\n",
    "    (\n",
    "        df_clean[\"duree_obtiention_autorisation\"].notna()\n",
    "        & (df_clean[\"duree_obtiention_autorisation\"] != 0)\n",
    "    ).sum(),\n",
    ")\n",
    "print(\"\\nShape du df nettoyé :\", df_clean.shape)\n",
    "print(\"Nombre de lignes dans le df nettoyé :\", len(df_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b531e9f0",
   "metadata": {},
   "source": [
    "# 3. Filtre et enrichissement des données\n",
    "Avec SITADEL, nous avons des données relatives au projet de construction (caractéristiques du demandeur, caractéristiques du projet), mais nous n'avons aucune information sur l'environnement territorial, à l'exception du code commune-département-région. Par appariement à l'aide du code commune, nous ajoutons donc la grille de densité qui nous renseigne sur le type d'occupation du sol (centre urbanisé, rural dense, rural peu dense, ...) et la population communale. Nous ajoutons également des données socio-démographiques du dossier complet : taux de pauvreté communal, part de ménages imposés, nombre de résidences secondaires, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74de567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "\n",
    "# 1. Chargement des autorisations nettoyées\n",
    "# Chemin a déjà été défini plus haut\n",
    "\n",
    "df = pd.read_parquet(chemin_data / \"autorisations.parquet\")\n",
    "\n",
    "df.info()\n",
    "print(df.head())\n",
    "\n",
    "# 2. Chargement de données externes\n",
    "# 2.1. Grille de densité à 7 niveaux\n",
    "grille_densite = pd.read_excel(\n",
    "    chemin_grilles,\n",
    "    skiprows=4\n",
    ")\n",
    "print(grille_densite.head())\n",
    "grille_densite.info()\n",
    "print(grille_densite.columns.tolist())\n",
    "\n",
    "cols_grille = [\n",
    "    \"CODGEO\",\n",
    "    \"DENS\",# De 1 à 7\n",
    "    \"PMUN17\" #Pop° municipale 2017\n",
    "]\n",
    "grille_reduite = grille_densite[cols_grille]\n",
    "\n",
    "# On corrige les formats\n",
    "df[\"COMM\"] = df[\"COMM\"].astype(str).str.zfill(5)\n",
    "grille_densite[\"CODGEO\"] = grille_densite[\"CODGEO\"].astype(str).str.zfill(5)\n",
    "\n",
    "# On fait un left-join (i.e. on enrichit les autorisations)\n",
    "df = df.merge(\n",
    "    grille_reduite,\n",
    "    how=\"left\",\n",
    "    left_on=\"COMM\",\n",
    "    right_on=\"CODGEO\",\n",
    "    validate=\"m:1\"\n",
    ")\n",
    "# On evalue les manquants\n",
    "missing_rate = df[\"DENS\"].isna().mean()\n",
    "print(f\"Pourcentage sans densité: {missing_rate:.2%}\")\n",
    "\n",
    "# 2.2. Dossier complet INSEE\n",
    "all_cols = pd.read_csv(chemin_dossier, sep=\";\", skiprows=0, nrows=1).columns.tolist()\n",
    "print(all_cols)\n",
    "\n",
    "cols_dossier_complet = [\n",
    "    \"CODGEO\", #code Insee\n",
    "    \"TP6021\",# taux pauvreté 60% en 2021 (retirée car trop de NA - secret statistique)\n",
    "    \"MED21\", #Médiane des revenus fiscaux en 2021\"\n",
    "    \"PIMP21\", #part de ménages fiscaux imposés (retirée car trop de NA - secret statistique)\n",
    "    \"PPEN21\", #Part des pensions dans le revenu fiscal (proxy pour concentration personnes âgées) (retirée car trop de NA - secret statistique)\n",
    "    \"DECE1621\", #nbr de deces entre 2016 et 2021\n",
    "    \"P16_LOG\", #nbr de logements dans la commune en 2022\n",
    "    \"P16_RP\", #RP en 2016\n",
    "    \"P16_RSECOCC\", #nbr de résidences secondaires et logements occasionnels en 2016\n",
    "    \"P16_LOGVAC\", #nbr de logements vacants en 2016 \n",
    "    \"P16_MAISON\", # Maisons individuelles en 2016\n",
    "    \"P16_APPART\",  #Appartements en 2016 \n",
    "    \"P16_NSCOL15P\", #Pop 15 ans ou plus non scolarisée en 2016\n",
    "    \"P16_ACTOCC15P\", #Actifs occupés 15 ans ou plus en 2016\n",
    "    \"P16_CHOM1564\", #Chômeurs 15-64 ans en 2016 (princ);\n",
    "]\n",
    "\n",
    "dossier_complet = pd.read_csv(\n",
    "    chemin_dossier,\n",
    "    sep=\";\",\n",
    "    encoding=\"utf-8\",\n",
    "    skiprows=0,\n",
    "    usecols=cols_dossier_complet\n",
    ")\n",
    "\n",
    "dossier_complet[\"CODGEO\"] = dossier_complet[\"CODGEO\"].astype(str).str.zfill(5)\n",
    "\n",
    "# Certaines colonnes devraient être numériques mais ne le sont pas à cause des infos de secret statistique\n",
    "cols_num = [\"MED21\"] #on ne met plus le taux de pauvreté, car trop de NA\n",
    "\n",
    "for col in cols_num:\n",
    "    dossier_complet[col] = (\n",
    "        dossier_complet[col]\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .replace(INSEE_secret_stat, None)\n",
    "        .str.replace(\"\\xa0\", \"\", regex=False)  # espace insécable\n",
    "        .str.replace(\" \", \"\", regex=False)\n",
    "        .str.replace(\",\", \".\", regex=False)\n",
    "        .astype(float)\n",
    "    )\n",
    "\n",
    "# On fait un left-join (i.e. on enrichit les autorisations)\n",
    "df = df.merge(\n",
    "    dossier_complet,\n",
    "    how=\"left\",\n",
    "    left_on=\"COMM\",\n",
    "    right_on=\"CODGEO\",\n",
    "    validate=\"m:1\"\n",
    ")\n",
    "\n",
    "df.info()\n",
    "df.to_parquet(\n",
    "    chemin_data / \"autorisations_enrichies.parquet\",\n",
    "    engine=\"pyarrow\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "# 3. Compte des valeurs manquantes pour l'ensemble des variables du dossier complet ajoutées\n",
    "variable_dict = {\n",
    "    # Identifiers\n",
    "    \"COMM\": \"Code INSEE de la commune (autorisations)\",\n",
    "    \"CODGEO\": \"Code INSEE de la commune (sources externes)\",\n",
    "\n",
    "    # Grille densité\n",
    "    \"DENS\": \"Niveau de densité communale (1 = très dense, 7 = très peu dense)\",\n",
    "    \"PMUN17\": \"Population municipale 2017\",\n",
    "\n",
    "    # Socio-éco (INSEE – dossier complet)\n",
    "    \"TP6021\": \"Taux de pauvreté à 60% du niveau de vie médian (2021)\",\n",
    "    \"MED21\": \"Médiane des revenus fiscaux (€) en 2021\",\n",
    "    \"PIMP21\": \"Part des ménages fiscaux imposés (%)\",\n",
    "    \"PPEN21\": \"Part des pensions dans le revenu fiscal (%)\",\n",
    "    \"DECE1621\": \"Nombre de décès cumulés entre 2016 et 2021\",\n",
    "    \"P16_LOG\": \"Nombre total de logements (2022)\",\n",
    "    \"P16_RP\": \"Nombre de résidences principales (2016)\",\n",
    "    \"P16_RSECOCC\": \"Résidences secondaires et logements occasionnels (2016)\",\n",
    "    \"P16_LOGVAC\": \"Logements vacants (2016)\",\n",
    "    \"P16_MAISON\": \"Maisons individuelles (2016)\",\n",
    "    \"P16_APPART\": \"Appartements (2016)\",\n",
    "    \"P16_NSCOL15P\": \"Population 15+ ans non scolarisée (2016)\",\n",
    "    \"P16_ACTOCC15P\": \"Actifs occupés 15+ ans (2016)\",\n",
    "    \"P16_CHOM1564\": \"Chômeurs 15–64 ans (2016)\"\n",
    "}\n",
    "\n",
    "vars_added = [v for v in variable_dict.keys() if v in df.columns]\n",
    "\n",
    "summary_table = (\n",
    "    pd.DataFrame({\n",
    "        \"variable\": vars_added,\n",
    "        \"description\": [variable_dict[v] for v in vars_added],\n",
    "        \"share_na\": [df[v].isna().mean() for v in vars_added]\n",
    "    })\n",
    "    .sort_values(\"share_na\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(summary_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbfcb3a",
   "metadata": {},
   "source": [
    "# 4. Prepocessing\n",
    "Le pré-traitement vise à construire un échantillon homogène, économiquement interprétable et compatible avec nos estimations, en premier lieu un lasso. À partir des autorisations enrichies, les observations sans délai d’ouverture de chantier sont exclues, tout comme les départements d’outre-mer afin de limiter l’hétérogénéité. L’analyse est restreinte à la période 2015–2019, afin de réduire les temps de calcul, et ne pas devoir prendre en compte le bousculement majeur qu'a été le premier confinement. Les annulations sont retirées et les délais sont élagués en fixant un plafond à deux ans (730 jours), entre le 95e et 99e percentile, afin de réduire l'influence des outliers. \n",
    "\n",
    "Enfin, les variables numériques sont standardisées afin de rendre les coefficients comparables sous pénalisation ℓ₁, tandis que les variables qualitatives sont encodées en indicatrices via un one-hot encoding. S'inspirant des codes des TD, l’ensemble du pré-traitement est intégré dans un pipeline sklearn. Nous conservons à la fin de ce prétraitement 530 000 observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dc4a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# LIBRAIRIES de sklearn\n",
    "# Pour le preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# A) Préparation des données pour le LASSO\n",
    "\n",
    "# Variables supplémentaires à retirer du jeu de données final\n",
    "vars_inutiles_delai_ouverture = [\n",
    "    \"COMM\",\n",
    "    \"REG_CODE\",\n",
    "    \"REG_LIBELLE\",\n",
    "    \"DEP_LIBELLE\",\n",
    "    \"NUM_DAU\",\n",
    "    \"APE_DEM\",\n",
    "    \"CJ_DEM\",\n",
    "    \"duree_obtiention_autorisation\",\n",
    "    \"DATE_REELLE_AUTORISATION\",\n",
    "    \"DATE_REELLE_DAACT\",\n",
    "    \"DATE_REELLE_DOC\",\n",
    "    \"DPC_AUT\",\n",
    "    \"DPC_PREM\",\n",
    "    \"DPC_DOC\",\n",
    "    \"DPC_DERN\", \n",
    "    \"duree_travaux\"]\n",
    "\n",
    "# Variables à traiter en One-Hot Encoding\n",
    "vars_categ = [\n",
    "    \"DEP_CODE\", \n",
    "    \"TYPE_DAU\", \n",
    "    \"ETAT_DAU\", \n",
    "    \"CAT_DEM\",\n",
    "    \"ZONE_OP\",\n",
    "    \"NATURE_PROJET_DECLAREE\",\n",
    "    \"UTILISATION\",\n",
    "    \"RES_PRINCIP_OU_SECOND\",\n",
    "    \"TYP_ANNEXE\",\n",
    "    \"RESIDENCE\" ] #on ne met pas la grille de densité pour ne pas perdre la nature hiérarchique de la variable (1 à 7)\n",
    "\n",
    "df = pd.read_parquet(chemin_data / \"autorisations_enrichies.parquet\")\n",
    "\n",
    "# Filtrage des régions et des années\n",
    "regions_outremer = [\n",
    "    \"Guadeloupe\", \"Martinique\", \"Guyane\",\n",
    "    \"La Réunion\", \"Mayotte\"\n",
    "]\n",
    "\n",
    "df_filtre_delai_ouverture = df.dropna(subset=[\"delai_ouverture_chantier\"])\n",
    "df_filtre_delai_ouverture = df_filtre_delai_ouverture[\n",
    "    ~df_filtre_delai_ouverture[\"REG_LIBELLE\"].isin(regions_outremer)\n",
    "]\n",
    "\n",
    "# On ne conserve que 2015-2019, et on retire les outliers (annulations, et délais dépassent 2 ans, entre 95 et 99e percentile)\n",
    "p95 = df_filtre_delai_ouverture[\"delai_ouverture_chantier\"].quantile(0.95)\n",
    "p99 = df_filtre_delai_ouverture[\"delai_ouverture_chantier\"].quantile(0.99)\n",
    "print(f\"Seuil 95e percentile : {p95:.1f} jours\")\n",
    "print(f\"Seuil 99e percentile : {p99:.1f} jours\")\n",
    "\n",
    "df_filtre_delai_ouverture = df_filtre_delai_ouverture[(df_filtre_delai_ouverture[\"annee_autorisation\"] >= 2015) & (df_filtre_delai_ouverture[\"annee_autorisation\"] <= 2019)]\n",
    "df_filtre_delai_ouverture = df_filtre_delai_ouverture[df_filtre_delai_ouverture[\"delai_ouverture_chantier\"] <= 730]\n",
    "df_filtre_delai_ouverture = df_filtre_delai_ouverture[df_filtre_delai_ouverture[\"ETAT_DAU\"] != 4] #on retire les annulations\n",
    "\n",
    "# Nettoyage et conversion des types sur l'échantillon\n",
    "df_filtre_delai_ouverture = df_filtre_delai_ouverture.drop(columns=vars_inutiles_delai_ouverture, errors=\"ignore\")\n",
    "\n",
    "# Suppression des lignes sans variable cible\n",
    "df_model = df_filtre_delai_ouverture.dropna(subset=[\"delai_ouverture_chantier\"])\n",
    "\n",
    "# On définit X et y\n",
    "y = df_model[\"delai_ouverture_chantier\"]\n",
    "X = df_model.drop(columns=[\"delai_ouverture_chantier\"])\n",
    "\n",
    "# Colonnes numériques\n",
    "num_cols = X.select_dtypes(\n",
    "    include=[\"float\", \"int\", \"bool\"]\n",
    ").columns.tolist()\n",
    "\n",
    "# Colonnes catégorielles\n",
    "cat_cols = X.select_dtypes(\n",
    "    include=[\"string\"]\n",
    ").columns.tolist()\n",
    "\n",
    "# On gère les NAs des variables explicatives\n",
    "cols_utiles = num_cols + cat_cols\n",
    "n_before_na = len(X)\n",
    "\n",
    "# Suppression des lignes avec NA sur les variables explicatives\n",
    "X = X.dropna(subset=cols_utiles)\n",
    "y = y.loc[X.index] \n",
    "\n",
    "n_after_na = len(X)\n",
    "drop_rate = 100 * (1 - n_after_na / n_before_na)\n",
    "\n",
    "print(f\"Observations après filtrage NA : {n_after_na:,}\")\n",
    "\n",
    "# B) Pipeline de pré-traitement et modèle LASSO\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Mise à l'échelle des variables numériques (StandardScaler)\n",
    "        (\"num\", StandardScaler(), num_cols), \n",
    "        # Encodage One-Hot des variables catégorielles\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\n",
    "    ],\n",
    "    remainder='drop' # On jette ce qui reste\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f32e43d",
   "metadata": {},
   "source": [
    "# 5. Régression LASSO\n",
    "Notre première estimation repose sur un modèle LASSO. La pénalisation ℓ₁, paramétrée ici par un coefficient de régularisation fixé à (\\alpha = 0{,}1) permet de limiter le sur-apprentissage et d’opérer une sélection automatique des variables en contraignant certains coefficients à zéro. Le modèle est entraîné sur l’échantillon d’apprentissage, puis évalué séparément sur les données de test à l’aide de métriques standards de régression (RMSE, MAE et (R^2)), afin de comparer les performances in-sample et out-of-sample. L’analyse est complétée par l’extraction des coefficients non nuls, commentés dans le rapport.\n",
    "\n",
    "Enfin, une spécification alternative est testée en modélisant le logarithme du délai, dans le but de réduire l’asymétrie de la variable cible (i.e. une queue de distribution épaisse) : celle-ci requiert une adaptation du paramètre alpha. Nous avons utilisé LassoCV, proche de GridSearchCV présenté en TD mais plus rapide pour notre cadre d'analyse, afin de trouver l'alpha optimal, plus petit que celui en niveau afin de ne pas augmenter la pénalisation. Les prédictions obtenues n'améliorent néanmoins pas notre estimation : la variance prédite semble resserée par la transformation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf8587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "lasso = Lasso(alpha=0.1, max_iter=10000)\n",
    "\n",
    "lasso_pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", lasso)\n",
    "])\n",
    "\n",
    "lasso_pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = lasso_pipeline.predict(X_train)\n",
    "y_test_pred = lasso_pipeline.predict(X_test)\n",
    "\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"RMSE train : {train_rmse:.2f}\")\n",
    "print(f\"RMSE test  : {test_rmse:.2f}\")\n",
    "print(f\"MAE train  : {train_mae:.2f}\")\n",
    "print(f\"MAE test   : {test_mae:.2f}\")\n",
    "print(f\"R² train   : {train_r2:.3f}\")\n",
    "print(f\"R² test    : {test_r2:.3f}\")\n",
    "\n",
    "print('R2 sur le test: {}'.format(lasso_pipeline.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "# Nombre de coefficients non nuls\n",
    "coefs = lasso_pipeline.named_steps[\"model\"].coef_\n",
    "nb_nonzero = np.sum(coefs != 0)\n",
    "print(\"Nombre de coefficients non nuls :\", nb_nonzero)\n",
    "\n",
    "# Liste des coefficients non nuls \n",
    "feature_names = (\n",
    "    list(num_cols) +\n",
    "    list(lasso_pipeline.named_steps[\"preprocess\"]\n",
    "         .named_transformers_[\"cat\"]\n",
    "         .get_feature_names_out(cat_cols))\n",
    ")\n",
    "coef_df = (\n",
    "    pd.DataFrame({\"variable\": feature_names, \"coef\": coefs})\n",
    "      .assign(abs_coef=lambda d: d.coef.abs())\n",
    ")\n",
    "print(\"\\nVariables les plus explicatives :\")\n",
    "with pd.option_context(\"display.max_rows\", None):\n",
    "    print(coef_df.query(\"coef != 0\").sort_values(\"abs_coef\", ascending=False))\n",
    "print(coef_df.query(\"coef != 0\").sort_values(\"abs_coef\", ascending=False))\n",
    "\n",
    "# Variables non explicatives\n",
    "print(\"\\nVariables éliminées par le LASSO :\")\n",
    "print(coef_df.query(\"coef == 0\").variable.tolist())\n",
    "\n",
    "\n",
    "#### Version avec transformation log de y ####\n",
    "y_log = np.log1p(y)\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(\n",
    "    X, y_log,\n",
    "    test_size=0.2,\n",
    "    random_state=36\n",
    ")\n",
    "\n",
    "# On doit modifier un peu notre pipeline, notamment pour l'hyperparamètre alpha: voir script plus bas\n",
    "lasso_log = Lasso(alpha=0.001, max_iter=10_000)\n",
    "\n",
    "lasso_log_pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", lasso_log)\n",
    "])\n",
    "\n",
    "lasso_log_pipeline.fit(X_train2, y_train2)\n",
    "\n",
    "# On calcule les prédictions du log\n",
    "y_train_log_pred = lasso_log_pipeline.predict(X_train2)\n",
    "y_test_log_pred = lasso_log_pipeline.predict(X_test2)\n",
    "\n",
    "# On revient à l'échelle originale pour la comparabilité\n",
    "y_train_pred = np.expm1(y_train_log_pred)\n",
    "y_test_pred = np.expm1(y_test_log_pred)\n",
    "\n",
    "y_train_true = np.expm1(y_train2)\n",
    "y_test_true = np.expm1(y_test2)\n",
    "\n",
    "# On calcule les métriques\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train_true, y_train_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_true, y_test_pred))\n",
    "\n",
    "train_mae = mean_absolute_error(y_train_true, y_train_pred)\n",
    "test_mae = mean_absolute_error(y_test_true, y_test_pred)\n",
    "\n",
    "train_r2 = r2_score(y_train_true, y_train_pred)\n",
    "test_r2 = r2_score(y_test_true, y_test_pred)\n",
    "\n",
    "print(\"\\n[LASSO – log(durée + 1), évalué sur l’échelle originale]\")\n",
    "print(f\"RMSE train : {train_rmse:.2f}\")\n",
    "print(f\"RMSE test  : {test_rmse:.2f}\")\n",
    "print(f\"MAE train  : {train_mae:.2f}\")\n",
    "print(f\"MAE test   : {test_mae:.2f}\")\n",
    "print(f\"R² train   : {train_r2:.3f}\")\n",
    "print(f\"R² test    : {test_r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c54ed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choix de l'hyperparamètre alpha \n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "lasso_cv = LassoCV(\n",
    "    alphas=np.logspace(-3, 0, 10),\n",
    "    cv=5,\n",
    "    max_iter=10_000,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lasso_log_pipeline = Pipeline([\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", lasso_cv)\n",
    "])\n",
    "\n",
    "lasso_log_pipeline.fit(X_train2, y_train2)\n",
    "\n",
    "print(\"Alpha optimal :\", lasso_cv.alpha_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b2e677",
   "metadata": {},
   "source": [
    "# 6. Random forest\n",
    "\n",
    "Nous conservons la même plage d’étude (2015–2019) et cherchons à améliorer le pouvoir prédictif du modèle en recourant à une méthode d’ensemble non paramétrique, la forêt aléatoire. Cela devrait nous permettre de capturer des relations non linéaires et des interactions complexes entre variables explicatives, difficilement prises en compte par les modèles linéaires.\n",
    "\n",
    "Le premier chunk nous permet de choisir les hyperparamètres du modèle à l’aide d’une procédure de recherche aléatoire avec élimination successive (*HalvingRandomSearchCV*). Cette méthode évalue un grand nombre de configurations sur un nombre limité de ressources, puis concentre progressivement l’effort de calcul sur les configurations les plus prometteuses. La ressource considérée ici est le nombre d’arbres de la forêt (*n_estimators*), augmenté itérativement de 50 à 350.\n",
    "\n",
    "La recherche porte sur trois hyperparamètres clés :\n",
    "- la profondeur maximale des arbres (*max_depth*), qui contrôle la complexité du modèle ;\n",
    "- le nombre minimal d’observations par feuille (*min_samples_leaf*), qui joue un rôle de régularisation ;\n",
    "- la proportion de variables considérées à chaque séparation (*max_features*), qui accroît la diversité des arbres.\n",
    "\n",
    "La performance des modèles est évaluée par validation croisée à trois plis, en utilisant le coefficient de détermination \\(R^2\\) comme fonction de score, conformément au cadre de régression étudié. Les hyperparamètres retenus correspondent à la configuration maximisant la performance moyenne en validation croisée.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b254b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va chercher à tuner automatiquement les hyperparamètres du modèle\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_dist = {\n",
    "    \"model__max_depth\": [6, 8, 10, 12],\n",
    "    \"model__min_samples_leaf\": [5, 10, 20, 50],\n",
    "    \"model__max_features\": [\"sqrt\", 0.3, 0.5],\n",
    "}\n",
    "\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", RandomForestRegressor(\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "\n",
    "search = HalvingRandomSearchCV(\n",
    "    rf_pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    resource=\"model__n_estimators\",\n",
    "    min_resources=50,   \n",
    "    max_resources = 350,\n",
    "    factor=3,\n",
    "    scoring=\"r2\",\n",
    "    cv=3,\n",
    "    random_state=36,\n",
    "    verbose=2,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\")\n",
    "print(search.best_params_)\n",
    "\n",
    "print(\"Best CV R²:\")\n",
    "print(search.best_score_)\n",
    "\n",
    "#Best parameters: {'model__min_samples_leaf': 10, 'model__max_features': 0.5, 'model__max_depth': 12, 'model__n_estimators': 150}\n",
    "# Best CV R²: 0.16358587685634499\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1cd050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", RandomForestRegressor(\n",
    "        max_depth=12, # Avons modifié la profondeur maximale\n",
    "        #min_samples_split=10,# à réduire, imho\n",
    "        min_samples_leaf=10, # également essayer de réduire\n",
    "        max_features=0.5,# uniquement sur notre dernière itération\n",
    "        n_estimators=150,\n",
    "        n_jobs=-1,\n",
    "        random_state=36\n",
    "    ))\n",
    "]) \n",
    "\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_rf = rf_pipeline.predict(X_train)\n",
    "y_test_pred_rf  = rf_pipeline.predict(X_test)\n",
    "\n",
    "train_rmse_rf = np.sqrt(mean_squared_error(y_train, y_train_pred_rf))\n",
    "test_rmse_rf  = np.sqrt(mean_squared_error(y_test, y_test_pred_rf))\n",
    "\n",
    "train_mae_rf = mean_absolute_error(y_train, y_train_pred_rf)\n",
    "test_mae_rf  = mean_absolute_error(y_test, y_test_pred_rf)\n",
    "\n",
    "train_r2_rf = r2_score(y_train, y_train_pred_rf)\n",
    "test_r2_rf  = r2_score(y_test, y_test_pred_rf)\n",
    "\n",
    "print(f\"RMSE train : {train_rmse_rf:.2f}\")\n",
    "print(f\"RMSE test  : {test_rmse_rf:.2f}\")\n",
    "print(f\"MAE train  : {train_mae_rf:.2f}\")\n",
    "print(f\"MAE test   : {test_mae_rf:.2f}\")\n",
    "print(f\"R² train   : {train_r2_rf:.3f}\")\n",
    "print(f\"R² test    : {test_r2_rf:.3f}\")\n",
    "\n",
    "print('Classification accuracy on test is: {}'.format(rf_pipeline.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6947dadc",
   "metadata": {},
   "source": [
    "# 7. Gradient boosting\n",
    "En application du cours d'Applied Statistical Learning, nous mobilisons une méthode de gradient boosting, qui repose sur une minimisation itérative du risque empirique par descente de gradient, chaque itération ajoutant un arbre faiblement profond afin de corriger les erreurs résiduelles du modèle précédent.\n",
    "Si le cours s’est principalement appuyé sur AdaBoost et XGBoost, nous retenons ici LightGBM, qui implémente le même principe théorique tout en introduisant des optimisations algorithmiques particulièrement adaptées aux grands échantillons et aux espaces de variables de dimension élevée.\n",
    "Compte tenu de la taille de notre base (>500 000 observations, et une centaine de features après prétraitement), LightGBM présente un coût computationnel nettement plus acceptable sur nos machines personnelles.\n",
    "\n",
    "\n",
    "https://blent.ai/blog/a/lightgbm-mieux-que-xgboost\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eeaa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# On reprend le dataset nettoyé et enrichi\n",
    "y = df_model[\"delai_ouverture_chantier\"]\n",
    "X = df_model.drop(columns=[\"delai_ouverture_chantier\"])\n",
    "\n",
    "# On retire les colonnes inutiles pour LGBM (identifiants et variables avec trop de NA, en object, ignorés par scikit mais pas lightgbm)\n",
    "cols_to_drop_lgbm = [\n",
    "    \"CODGEO_x\",\n",
    "    \"CODGEO_y\",\n",
    "    \"TP6021\",\n",
    "    \"PIMP21\",\n",
    "    \"PPEN21\",\n",
    "]\n",
    "X = X.drop(columns=[c for c in cols_to_drop_lgbm if c in X.columns])\n",
    "\n",
    "cat_cols = X.select_dtypes(include=[\"string\", \"object\"]).columns.tolist()\n",
    "for c in cat_cols:\n",
    "    X[c] = X[c].astype(\"category\")\n",
    "\n",
    "num_cols = X.select_dtypes(include=[\"float\", \"int\", \"bool\"]).columns.tolist()\n",
    "\n",
    "# On crée des jeux d'entraînement  et test\n",
    "X_train, X_tmp, y_train, y_tmp = train_test_split(X, y, test_size=0.20, random_state=36)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_tmp, y_tmp, test_size=0.50, random_state=36)\n",
    "\n",
    "# Passons au modèle : regression L1 ou poisson; \n",
    "model = LGBMRegressor(\n",
    "    boosting_type=\"gbdt\",\n",
    "    objective=\"regression_l1\",\n",
    "    n_estimators=5000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=127,\n",
    "    min_child_samples=50,\n",
    "    subsample=0.8,\n",
    "    subsample_freq=1,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Early stopping = callback + eval_set obligatoire. :contentReference[oaicite:3]{index=3}\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric=\"mae\",\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=200),\n",
    "        lgb.log_evaluation(period=50)\n",
    "    ],\n",
    ")\n",
    "\n",
    "# best_iteration_ est renseigné si early_stopping() est utilisé. :contentReference[oaicite:4]{index=4}\n",
    "y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MAE  : {mae:.2f} jours\")\n",
    "print(f\"RMSE : {rmse:.2f} jours\")\n",
    "print(f\"R²   : {r2:.3f}\")\n",
    "\n",
    "# Imprimons l'importance des variables\n",
    "imp = pd.Series(model.feature_importances_, index=model.feature_name_).sort_values(ascending=False)\n",
    "print(imp.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab17f8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q lightgbm\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# On réutilise le même preprocess que précédemment\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=0.2,\n",
    "    random_state=36\n",
    ")\n",
    "\n",
    "lgbm = LGBMRegressor(\n",
    "    objective=\"regression\",\n",
    "    n_estimators=5000,            # assez grand → early stopping\n",
    "    learning_rate=0.03,           # plus lent mais plus stable\n",
    "    num_leaves=63,                # complexité contrôlée (≈ arbre depth 7–8)\n",
    "    max_depth=-1,                 # laisse num_leaves gouverner\n",
    "    min_child_samples=100,        # régularisation adaptée gros N\n",
    "    subsample=0.8,\n",
    "    subsample_freq=1,\n",
    "    colsample_bytree=0.7,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lgbm_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"model\", lgbm)\n",
    "    ]\n",
    ")\n",
    "\n",
    "lgbm_pipeline.fit(\n",
    "    X_tr,\n",
    "    y_tr,\n",
    "    model__eval_set=[(X_val, y_val)],\n",
    "    model__eval_metric=\"rmse\",\n",
    "    model__callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=100),\n",
    "        lgb.log_evaluation(period=50)\n",
    "    ]\n",
    ")\n",
    "\n",
    "y_train_pred = lgbm_pipeline.predict(X_tr)\n",
    "y_val_pred   = lgbm_pipeline.predict(X_val)\n",
    "y_test_pred  = lgbm_pipeline.predict(X_test)\n",
    "\n",
    "rmse_train = mean_squared_error(y_tr, y_train_pred, squared=False)\n",
    "rmse_val   = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "rmse_test  = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "\n",
    "print(f\"RMSE train : {rmse_train:.2f}\")\n",
    "print(f\"RMSE valid : {rmse_val:.2f}\")\n",
    "print(f\"RMSE test  : {rmse_test:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef7565c",
   "metadata": {},
   "source": [
    "# 8. Réseau de neurones\n",
    "\n",
    "Dans cette section, nous mobilisons un modèle de réseau de neurones artificiels de type Multilayer Perceptron afin d’évaluer si une flexibilité accrue permet d’améliorer la prédiction des délais de mise en chantier. Nous commençons par évaluer les performances d'un premier modèles dont les hyperparamètres sont fixés de manière relativement arbitraire quoique logiques : l'idée est d'abord de ne pas complexifier outre mesure l'architecture du modèle avec de trop nombreuses couches et de lui permettre de tourner en un temps raisonnable. \n",
    "\n",
    "## A) Modèle principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e695d2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp_pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", MLPRegressor(\n",
    "        hidden_layer_sizes=(32, 16),   \n",
    "        activation=\"relu\",\n",
    "        solver=\"adam\",\n",
    "        alpha=1e-4,\n",
    "        learning_rate=\"adaptive\",\n",
    "        learning_rate_init=0.001,\n",
    "        batch_size=256,               \n",
    "        max_iter=300,                  \n",
    "        early_stopping=True,\n",
    "        n_iter_no_change=15,\n",
    "        tol=1e-4,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "mlp_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Prédictions\n",
    "y_train_pred_mlp = mlp_pipeline.predict(X_train)\n",
    "y_test_pred_mlp  = mlp_pipeline.predict(X_test)\n",
    "\n",
    "# Métriques\n",
    "train_rmse_mlp = np.sqrt(mean_squared_error(y_train, y_train_pred_mlp))\n",
    "test_rmse_mlp  = np.sqrt(mean_squared_error(y_test, y_test_pred_mlp))\n",
    "\n",
    "train_mae_mlp = mean_absolute_error(y_train, y_train_pred_mlp)\n",
    "test_mae_mlp  = mean_absolute_error(y_test, y_test_pred_mlp)\n",
    "\n",
    "train_r2_mlp = r2_score(y_train, y_train_pred_mlp)\n",
    "test_r2_mlp  = r2_score(y_test, y_test_pred_mlp)\n",
    "\n",
    "print(f\"[MLP] RMSE train : {train_rmse_mlp:.2f}\")\n",
    "print(f\"[MLP] RMSE test  : {test_rmse_mlp:.2f}\")\n",
    "print(f\"[MLP] MAE train  : {train_mae_mlp:.2f}\")\n",
    "print(f\"[MLP] MAE test   : {test_mae_mlp:.2f}\")\n",
    "print(f\"[MLP] R² train   : {train_r2_mlp:.3f}\")\n",
    "print(f\"[MLP] R² test    : {test_r2_mlp:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11234ad2",
   "metadata": {},
   "source": [
    "## B) Tests de plusieurs hyperparamètres\n",
    "\n",
    "Nous testons ici d'autres valeurs d'hyperparamètres. Les résultats sont dans le rapport car il faut savoir que le code met environ 15 minutes à tourner. L'amélioration des résultats avec ces différentes configurations est de toutes façons marginale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1f6507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "hls_grid = [(32, 16), (64, 32), (64, 32, 16)]\n",
    "alpha_grid = [1e-4, 1e-3]\n",
    "lr_init_grid = [1e-3, 5e-4]\n",
    "\n",
    "results = []\n",
    "\n",
    "for hls in hls_grid:\n",
    "    for alpha in alpha_grid:\n",
    "        for lr_init in lr_init_grid:\n",
    "            mlp_pipe = Pipeline(steps=[\n",
    "                (\"preprocess\", preprocess),\n",
    "                (\"model\", MLPRegressor(\n",
    "                    hidden_layer_sizes=hls,\n",
    "                    activation=\"relu\",\n",
    "                    solver=\"adam\",\n",
    "                    alpha=alpha,\n",
    "                    learning_rate=\"adaptive\",\n",
    "                    learning_rate_init=lr_init,\n",
    "                    batch_size=256,\n",
    "                    max_iter=300,\n",
    "                    early_stopping=True,\n",
    "                    n_iter_no_change=15,\n",
    "                    tol=1e-4,\n",
    "                    random_state=42\n",
    "                ))\n",
    "            ])\n",
    "\n",
    "            mlp_pipe.fit(X_train, y_train)\n",
    "\n",
    "            y_train_pred = mlp_pipe.predict(X_train)\n",
    "            y_test_pred  = mlp_pipe.predict(X_test)\n",
    "\n",
    "            rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "            rmse_test  = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "            mae_train  = mean_absolute_error(y_train, y_train_pred)\n",
    "            mae_test   = mean_absolute_error(y_test, y_test_pred)\n",
    "            r2_train   = r2_score(y_train, y_train_pred)\n",
    "            r2_test    = r2_score(y_test, y_test_pred)\n",
    "\n",
    "            results.append({\n",
    "                \"hidden_layer_sizes\": hls,\n",
    "                \"alpha\": alpha,\n",
    "                \"learning_rate_init\": lr_init,\n",
    "                \"RMSE_train\": rmse_train,\n",
    "                \"RMSE_test\": rmse_test,\n",
    "                \"MAE_train\": mae_train,\n",
    "                \"MAE_test\": mae_test,\n",
    "                \"R2_train\": r2_train,\n",
    "                \"R2_test\": r2_test\n",
    "            })\n",
    "\n",
    "# Tri par RMSE test (ou MAE test)\n",
    "results_sorted = sorted(results, key=lambda d: d[\"RMSE_test\"])\n",
    "\n",
    "for r in results_sorted[:8]:\n",
    "    print(\n",
    "        f\"hls={r['hidden_layer_sizes']}, alpha={r['alpha']}, lr_init={r['learning_rate_init']} | \"\n",
    "        f\"RMSE tr={r['RMSE_train']:.2f}, te={r['RMSE_test']:.2f} | \"\n",
    "        f\"R2 tr={r['R2_train']:.3f}, te={r['R2_test']:.3f}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
