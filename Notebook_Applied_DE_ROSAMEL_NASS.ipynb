{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efb09aad",
   "metadata": {},
   "source": [
    "# Applied Statistical Learning - Prédiction de la durée de mise en chantier en France\n",
    "Dans le cadre de ce projet, nous avons exploité les données du fichier SITADEL, qui recense de manière exhaustive l'ensemble des permis de construire en France. Pour avoir un échantillon homogène, nous nous sommes concentrés sur la période 2015-2019 (i.e. avant les délais liés au Covid). Nous avons enrichi notre fichier à l'aide de deux sources externes: le fichier complet de l'Insee, agrégeant des données en open data pour l'ensemble des communes, ainsi que la grille de densité des communes de l'Insee. \n",
    "Pour faciliter la réplication de nos résultats, nous avons déposé nos données brutes sur une dropbox (1.5Go) : https://www.dropbox.com/scl/fo/b3dw5eht79785mrg7uueq/APQ7m21mQcsZzh7Uy7ASj6k?rlkey=rutboi86sxni30nqg6lzsycqh&st=n60gni1d&dl=0. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04dcffa",
   "metadata": {},
   "source": [
    "# 1. Mise en place des chemins\n",
    "Placer l'ensemble des fichiers de la dropbox dans un dossier data, puis adapter le chemin ci-dessous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8319cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# CHEMIN A ADAPTER\n",
    "chemin_data = Path(\"chemin\")   # mettre le chemin local vers le dossier data avec les fichiers de la dropbox\n",
    "\n",
    "# Suffixes fixes\n",
    "chemin_autorisation = chemin_data / \"Liste-des-autorisations-durbanisme-creant-des-logements.2025-10.csv\"\n",
    "chemin_grilles = chemin_data / \"grille_densite_7_niveaux_2019.xlsx\"\n",
    "chemin_dossier = chemin_data / \"dossier_complet.csv\"\n",
    "\n",
    "# installer pyarrow pour lire les fichiers parquet\n",
    "%pip install pyarrow fastparquet\n",
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1bf53f",
   "metadata": {},
   "source": [
    "# 2. Chargement des données\n",
    "Ce premier chunk permet de sélectionner nos variables d'intérêt, et d'observer notre base avant tout filtre. Le fichier contient initialement 1,8 million de permis de construire, couvrant souvent plusieurs logements. Plusieurs variables d'intérêt sont possibles : la durée avant l'obtention de l'autorisation, la durée entre l'obtention de l'autorisation et la mise en chantier, ou la durée du chantier. Afin d'éviter de nous restreindre à des chantiers terminés, nous étudierons la **durée de mise en chantier**. Après le retrait de durées absentes ou négatives, nous avons 1,7 million de permis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28025f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CHARGEMENT DES DONNEES ####\n",
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Chargement de Sitadel\n",
    "# Chemin : défini en amont, vérifier que le chunk a bien été executé. \n",
    "\n",
    "# 1.1 Exclusion ex ante des colonnes non pertinentes\n",
    "all_cols = pd.read_csv(chemin_autorisation, sep=\";\", skiprows=1, nrows=1).columns.tolist()\n",
    "\n",
    "vars_mai2022 = [ #On retire les colonnes qui n'existent qu'à partir de mai 2022\n",
    "    \"AN_DEPOT\",  # \"DPC_PREM\", (theoriquement il faudrait la retirer, mais bon)\n",
    "    \"NATURE_PROJET_COMPLETEE\",\n",
    "    \"DESTINATION_PRINCIPALE\",\n",
    "    \"TYPE_PRINCIP_LOGTS_CREES\",\n",
    "    \"TYPE_TRANSFO_PRINCIPAL\",\n",
    "    \"TYPE_PRINCIP_LOCAUX_TRANSFORMES\",\n",
    "    \"I_PISCINE\",\n",
    "    \"I_GARAGE\",\n",
    "    \"I_VERANDA\",\n",
    "    \"I_ABRI_JARDIN\",\n",
    "    \"I_AUTRE_ANNEXE\",\n",
    "    \"RES_PERS_AGEES\",\n",
    "    \"RES_ETUDIANTS\",\n",
    "    \"RES_TOURISME\",\n",
    "    \"RES_HOTEL_SOCIALE\",\n",
    "    \"RES_SOCIALE\",\n",
    "    \"RES_HANDICAPES\",\n",
    "    \"RES_AUTRE\",\n",
    "    \"NB_LGT_INDIV_PURS\",\n",
    "    \"NB_LGT_INDIV_GROUPES\",\n",
    "    \"NB_LGT_RES\",\n",
    "    \"NB_LGT_COL_HORS_RES\",\n",
    "    \"SURF_HEB_TRANSFORMEE\",\n",
    "    \"SURF_BUR_TRANSFORMEE\",\n",
    "    \"SURF_COM_TRANSFORMEE\",\n",
    "    \"SURF_ART_TRANSFORMEE\",\n",
    "    \"SURF_IND_TRANSFORMEE\",\n",
    "    \"SURF_AGR_TRANSFORMEE\",\n",
    "    \"SURF_ENT_TRANSFORMEE\",\n",
    "    \"SURF_PUB_TRANSFORMEE\",\n",
    "]\n",
    "vars_non_pertinentes = [\n",
    "    \"Num_DAU\",\n",
    "    \"SIREN_DEM\",\n",
    "    \"SIRET_DEM\",\n",
    "    \"DENOM_DEM\",\n",
    "    \"CODPOST_DEM\",\n",
    "    \"LOCALITE_DEM\",\n",
    "    \"ADR_NUM_TER\",\n",
    "    \"ADR_TYPEVOIE_TER\",\n",
    "    \"ADR_LIBVOIE_TER\",\n",
    "    \"ADR_LIEUDIT_TER\",\n",
    "    \"ADR_LOCALITE_TER\",\n",
    "    \"ADR_CODPOST_TER\",\n",
    "    \"SEC_CADASTRE1\",\n",
    "    \"NUM_CADASTRE1\",\n",
    "    \"SEC_CADASTRE2\",\n",
    "    \"NUM_CADASTRE2\",\n",
    "    \"SEC_CADASTRE3\",\n",
    "    \"NUM_CADASTRE3\",\n",
    "]\n",
    "\n",
    "cols_to_drop = set(vars_mai2022 + vars_non_pertinentes)\n",
    "use_cols = [c for c in all_cols if c not in cols_to_drop]\n",
    "\n",
    "# 1.2 Chargement avec les colonnes filtrées\n",
    "df = pd.read_csv(\n",
    "    chemin_autorisation,\n",
    "    sep=\";\",\n",
    "    encoding=\"utf-8\",\n",
    "    skiprows=1,\n",
    "    usecols=use_cols\n",
    ")\n",
    "\n",
    "# 3. Etudions nos dates\n",
    "date_cols = [\n",
    "    \"DATE_REELLE_AUTORISATION\",\n",
    "    \"DATE_REELLE_DOC\",\n",
    "    \"DPC_AUT\",\n",
    "    \"DATE_REELLE_DAACT\",\n",
    "    \"DPC_PREM\",\n",
    "]\n",
    "\n",
    "for col in date_cols:\n",
    "    if col in df.columns:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Type: {df[col].dtype}\")\n",
    "        print(\"  Sample values:\")\n",
    "        print(df[col].head(10).tolist())\n",
    "        print(f\"  Null count: {df[col].isna().sum()}\")\n",
    "    else:\n",
    "        print(f\"\\n{col}: NOT FOUND in dataframe\")\n",
    "\n",
    "\n",
    "## Puis, nettoyons le dataset\n",
    "def nettoyer_dataset(df: pd.DataFrame):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Conversion des dates en datetime\n",
    "    # 3 dates en format DD/MM/YYYY\n",
    "    dmY_cols = [\"DATE_REELLE_AUTORISATION\", \"DATE_REELLE_DOC\", \"DATE_REELLE_DAACT\"]\n",
    "    for col in dmY_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(\n",
    "                df[col], errors=\"coerce\", dayfirst=True\n",
    "            )\n",
    "\n",
    "    # 2 dates en format YYYY-MM\n",
    "    for col in [\"DPC_AUT\", \"DPC_PREM\"]:\n",
    "        if col in df.columns:\n",
    "            # ensure strings and strip whitespace, coerce bad values\n",
    "            df[col] = pd.to_datetime(\n",
    "                df[col].astype(str).str.strip(), errors=\"coerce\", format=\"%Y-%m\"\n",
    "            )\n",
    "\n",
    "    # On retire les dates fausses\n",
    "    min_date = pd.Timestamp(\"1900-01-01\") \n",
    "    max_date = pd.Timestamp(\"2025-12-31\")\n",
    "    for col in dmY_cols + [\"DPC_AUT\", \"DPC_PREM\"]:\n",
    "        if col in df.columns:\n",
    "            mask = (df[col] < min_date) | (df[col] > max_date)\n",
    "            df.loc[mask, col] = pd.NaT\n",
    "\n",
    "    # On construit trois variables cibles, même si in fine on n'utilisera que delai_ouverture_chantier\n",
    "    if \"DATE_REELLE_AUTORISATION\" in df.columns and \"DATE_REELLE_DOC\" in df.columns:\n",
    "        mask = df[\"DATE_REELLE_AUTORISATION\"].notna() & df[\"DATE_REELLE_DOC\"].notna()\n",
    "        df.loc[mask, \"delai_ouverture_chantier\"] = (\n",
    "            df.loc[mask, \"DATE_REELLE_DOC\"] - df.loc[mask, \"DATE_REELLE_AUTORISATION\"]\n",
    "        ).dt.days\n",
    "\n",
    "    if \"DATE_REELLE_DAACT\" in df.columns and \"DATE_REELLE_DOC\" in df.columns:\n",
    "        mask = df[\"DATE_REELLE_DAACT\"].notna() & df[\"DATE_REELLE_DOC\"].notna()\n",
    "        df.loc[mask, \"duree_travaux\"] = (\n",
    "            df.loc[mask, \"DATE_REELLE_DAACT\"] - df.loc[mask, \"DATE_REELLE_DOC\"]\n",
    "        ).dt.days\n",
    "\n",
    "    if \"DPC_AUT\" in df.columns and \"DPC_PREM\" in df.columns:\n",
    "        mask = df[\"DPC_PREM\"].notna() & df[\"DPC_AUT\"].notna()\n",
    "        df.loc[mask, \"duree_obtiention_autorisation\"] = (\n",
    "            df.loc[mask, \"DPC_AUT\"] - df.loc[mask, \"DPC_PREM\"]\n",
    "        ).dt.days\n",
    "\n",
    "    # On traite les variables cibles: conversion en numérique et suppression des valeurs négatives\n",
    "    duration_cols = [\n",
    "        \"delai_ouverture_chantier\",\n",
    "        \"duree_travaux\",\n",
    "        \"duree_obtiention_autorisation\",\n",
    "    ]\n",
    "    for col in duration_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "            df.loc[df[col] <= 0, col] = pd.NA\n",
    "\n",
    "    # Suppression des lignes sans cibles valides (NA, null, zero or negative)\n",
    "    existing_duration_cols = [c for c in duration_cols if c in df.columns]\n",
    "    df = df.dropna(subset=existing_duration_cols, how=\"all\")\n",
    "    \n",
    "    # On crée mois et année\n",
    "    df[\"annee_autorisation\"] = df[\"DATE_REELLE_AUTORISATION\"].dt.year\n",
    "    df[\"mois_autorisation\"] = df[\"DATE_REELLE_AUTORISATION\"].dt.month\n",
    "    return df\n",
    "\n",
    "\n",
    "df_clean = nettoyer_dataset(df)\n",
    "\n",
    "# On règle également les codes\n",
    "colonnes_codes = [\n",
    "    \"DEP_CODE\",\n",
    "    \"COMM\",\n",
    "    \"CODGEO\",\n",
    "    \"REG_CODE\"\n",
    "]\n",
    "\n",
    "for col in colonnes_codes:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = df_clean[col].astype(\"string\")\n",
    "\n",
    "\n",
    "df_clean.to_parquet(\n",
    "    chemin_data / \"autorisations.parquet\",\n",
    "    engine=\"pyarrow\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "## 4. Comparaison\n",
    "# Statistics before cleaning\n",
    "print(\"Original df shape:\", df.shape)\n",
    "print(\"Total lines in original df:\", len(df))\n",
    "\n",
    "# Statistics after cleaning\n",
    "print(\"\\nAfter cleaning:\")\n",
    "print(\n",
    "    \"Lines with non-NA delai_ouverture_chantier:\",\n",
    "    df_clean[\"delai_ouverture_chantier\"].notna().sum(),\n",
    ")\n",
    "print(\"Lines with non-NA duree_travaux:\", df_clean[\"duree_travaux\"].notna().sum())\n",
    "print(\n",
    "    \"Lines with non-NA duree_obtiention_autorisation:\",\n",
    "    df_clean[\"duree_obtiention_autorisation\"].notna().sum(),\n",
    ")\n",
    "print(\n",
    "    \"Lines with non-0 duree_obtiention_autorisation:\",\n",
    "    (\n",
    "        df_clean[\"duree_obtiention_autorisation\"].notna()\n",
    "        & (df_clean[\"duree_obtiention_autorisation\"] != 0)\n",
    "    ).sum(),\n",
    ")\n",
    "print(\"\\nFinal cleaned df shape:\", df_clean.shape)\n",
    "print(\"Final cleaned df lines:\", len(df_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b531e9f0",
   "metadata": {},
   "source": [
    "# 3. Filtre et enrichissement des données\n",
    "Avec SITADEL, nous avons des données relatives au projet de construction (caractéristiques du demandeur, caractéristiques du projet), mais nous n'avons aucune information sur l'environnement territorial, à l'exception du code commune-département-région. Nous ajoutons donc la grille de densité, nous renseignant sur le type d'occupation du sol (centre urbanisé, rural dense, rural peu dense, ...) et la population communale. Nous ajoutons également des données socio-démographiques du dossier complet : taux de pauvreté communal, part de ménages imposés, nombre de résidences secondaires, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74de567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "\n",
    "# 1. Chargement des autorisations nettoyées\n",
    "# Chemin a déjà été défini plus haut\n",
    "\n",
    "df = pd.read_parquet(chemin_data / \"autorisations.parquet\")\n",
    "\n",
    "df.info()\n",
    "print(df.head())\n",
    "\n",
    "# 2. Chargement de données externes\n",
    "# 2.1. Grille de densité à 7 niveaux\n",
    "grille_densite = pd.read_excel(\n",
    "    chemin_grilles,\n",
    "    skiprows=4\n",
    ")\n",
    "print(grille_densite.head())\n",
    "grille_densite.info()\n",
    "print(grille_densite.columns.tolist())\n",
    "\n",
    "cols_grille = [\n",
    "    \"CODGEO\",\n",
    "    \"DENS\",# De 1 à 7\n",
    "    \"PMUN17\" #Pop° municipale 2017\n",
    "]\n",
    "grille_reduite = grille_densite[cols_grille]\n",
    "\n",
    "# On corrige les formats\n",
    "df[\"COMM\"] = df[\"COMM\"].astype(str).str.zfill(5)\n",
    "grille_densite[\"CODGEO\"] = grille_densite[\"CODGEO\"].astype(str).str.zfill(5)\n",
    "df = df.merge(\n",
    "    grille_reduite,\n",
    "    how=\"left\",\n",
    "    left_on=\"COMM\",\n",
    "    right_on=\"CODGEO\",\n",
    "    validate=\"m:1\"\n",
    ")\n",
    "\n",
    "missing_rate = df[\"DENS\"].isna().mean()\n",
    "print(f\"Pourcentage sans densité: {missing_rate:.2%}\")\n",
    "\n",
    "\n",
    "# 2.2. Données socio-économiques par communes\n",
    "\n",
    "all_cols = pd.read_csv(chemin_dossier, sep=\";\", skiprows=0, nrows=1).columns.tolist()\n",
    "print(all_cols)\n",
    "\n",
    "cols_dossier_complet = [\n",
    "    \"CODGEO\",\n",
    "    \"TP6021\",# taux pauvreté 60% en 2021\n",
    "    \"MED21\", #Médiane des revenus fiscaux en 2021\"\n",
    "    \"PIMP21\", #part de ménages fiscaux imposés\n",
    "    \"PPEN21\", #Part des pensions dans le revenu fiscal (proxy pour concentration personnes âgées)\n",
    "    \"DECE1621\", #nbr de deces entre 2016 et 2021\n",
    "    \"P16_LOG\", #nbr de logements dans la commune en 2022\n",
    "    \"P16_RP\", #RP en 2016\n",
    "    \"P16_RSECOCC\", #nbr de résidences secondaires et logements occasionnels en 2016\n",
    "    \"P16_LOGVAC\", #nbr de logements vacants en 2016 \n",
    "    \"P16_MAISON\",\n",
    "    \"P16_APPART\",  #Appartements en 2016 \n",
    "    \"P16_NSCOL15P\", #Pop 15 ans ou plus non scolarisée en 2016\n",
    "    \"P16_ACTOCC15P\", #Actifs occupés 15 ans ou plus en 2016\n",
    "    \"P16_CHOM1564\", #Chômeurs 15-64 ans en 2016 (princ);\n",
    "]\n",
    "\n",
    "dossier_complet = pd.read_csv(\n",
    "    chemin_dossier,\n",
    "    sep=\";\",\n",
    "    encoding=\"utf-8\",\n",
    "    skiprows=0,\n",
    "    usecols=cols_dossier_complet\n",
    ")\n",
    "\n",
    "dossier_complet[\"CODGEO\"] = dossier_complet[\"CODGEO\"].astype(str).str.zfill(5)\n",
    "df = df.merge(\n",
    "    dossier_complet,\n",
    "    how=\"left\",\n",
    "    left_on=\"COMM\",\n",
    "    right_on=\"CODGEO\",\n",
    "    validate=\"m:1\"\n",
    ")\n",
    "\n",
    "# 3. Compte des valeurs manquantes\n",
    "variable_dict = {\n",
    "    # Identifiers\n",
    "    \"COMM\": \"Code INSEE de la commune (autorisations)\",\n",
    "    \"CODGEO\": \"Code INSEE de la commune (sources externes)\",\n",
    "\n",
    "    # Grille densité\n",
    "    \"DENS\": \"Niveau de densité communale (1 = très dense, 7 = très peu dense)\",\n",
    "    \"PMUN17\": \"Population municipale 2017\",\n",
    "\n",
    "    # Socio-éco (INSEE – dossier complet)\n",
    "    \"TP6021\": \"Taux de pauvreté à 60% du niveau de vie médian (2021)\",\n",
    "    \"MED21\": \"Médiane des revenus fiscaux (€) en 2021\",\n",
    "    \"PIMP21\": \"Part des ménages fiscaux imposés (%)\",\n",
    "    \"PPEN21\": \"Part des pensions dans le revenu fiscal (%)\",\n",
    "    \"DECE1621\": \"Nombre de décès cumulés entre 2016 et 2021\",\n",
    "    \"P16_LOG\": \"Nombre total de logements (2022)\",\n",
    "    \"P16_RP\": \"Nombre de résidences principales (2016)\",\n",
    "    \"P16_RSECOCC\": \"Résidences secondaires et logements occasionnels (2016)\",\n",
    "    \"P16_LOGVAC\": \"Logements vacants (2016)\",\n",
    "    \"P16_MAISON\": \"Maisons individuelles (2016)\",\n",
    "    \"P16_APPART\": \"Appartements (2016)\",\n",
    "    \"P16_NSCOL15P\": \"Population 15+ ans non scolarisée (2016)\",\n",
    "    \"P16_ACTOCC15P\": \"Actifs occupés 15+ ans (2016)\",\n",
    "    \"P16_CHOM1564\": \"Chômeurs 15–64 ans (2016)\"\n",
    "}\n",
    "\n",
    "vars_added = [v for v in variable_dict.keys() if v in df.columns]\n",
    "\n",
    "summary_table = (\n",
    "    pd.DataFrame({\n",
    "        \"variable\": vars_added,\n",
    "        \"description\": [variable_dict[v] for v in vars_added],\n",
    "        \"share_na\": [df[v].isna().mean() for v in vars_added]\n",
    "    })\n",
    "    .sort_values(\"share_na\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(summary_table)\n",
    "df_clean.to_parquet(\n",
    "    chemin_data / \"autorisations_enrichies.parquet\",\n",
    "    engine=\"pyarrow\",\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbfcb3a",
   "metadata": {},
   "source": [
    "# 4. Prepocessing\n",
    "Nous pouvons désormais nous intéresser à la plage d'étude (2015-2019), afin de réduire les temps de calcul, et ne pas devoir prendre en compte le bousculement majeur qu'a été le premier confinement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dc4a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# A) Préparation des données pour le LASSO\n",
    "\n",
    "# Variables supplémentaires à retirer du jeu de données final\n",
    "vars_inutiles_delai_ouverture = [\n",
    "    \"COMM\",\n",
    "    \"REG_CODE\",\n",
    "    \"REG_LIBELLE\",\n",
    "    \"DEP_LIBELLE\",\n",
    "    \"NUM_DAU\",\n",
    "    \"APE_DEM\",\n",
    "    \"CJ_DEM\",\n",
    "    \"duree_obtiention_autorisation\",\n",
    "    \"DATE_REELLE_AUTORISATION\",\n",
    "    \"DATE_REELLE_DAACT\",\n",
    "    \"DATE_REELLE_DOC\",\n",
    "    \"DPC_AUT\",\n",
    "    \"DPC_PREM\",\n",
    "    \"DPC_DOC\",\n",
    "    \"DPC_DERN\", \n",
    "    \"duree_travaux\"]\n",
    "\n",
    "# Variables à traiter en One-Hot Encoding\n",
    "vars_categ = [\n",
    "    \"DEP_CODE\", \n",
    "    \"TYPE_DAU\", \n",
    "    \"ETAT_DAU\", \n",
    "    \"CAT_DEM\",\n",
    "    \"ZONE_OP\",\n",
    "    \"NATURE_PROJET_DECLAREE\",\n",
    "    \"UTILISATION\",\n",
    "    \"RES_PRINCIP_OU_SECOND\",\n",
    "    \"TYP_ANNEXE\",\n",
    "    \"RESIDENCE\" ]\n",
    "\n",
    "df = pd.read_parquet(chemin_data / \"autorisations_enrichies.parquet\")\n",
    "\n",
    "# Filtrage des régions et des années\n",
    "regions_outremer = [\n",
    "    \"Guadeloupe\", \"Martinique\", \"Guyane\",\n",
    "    \"La Réunion\", \"Mayotte\"\n",
    "]\n",
    "\n",
    "df[\"DATE_REELLE_AUTORISATION\"] = pd.to_datetime(df[\"DATE_REELLE_AUTORISATION\"], errors=\"coerce\")\n",
    "df[\"annee_autorisation\"] = df[\"DATE_REELLE_AUTORISATION\"].dt.year\n",
    "df[\"mois_autorisation\"] = df[\"DATE_REELLE_AUTORISATION\"].dt.month\n",
    "\n",
    "# Filtrage des lignes sans la variable cible\n",
    "df_filtre_delai_ouverture = df.dropna(subset=[\"delai_ouverture_chantier\"])\n",
    "df_filtre_delai_ouverture = df_filtre_delai_ouverture[\n",
    "    ~df_filtre_delai_ouverture[\"REG_LIBELLE\"].isin(regions_outremer)\n",
    "]\n",
    "\n",
    "# On ne conserve que 2015-2019, et on retire les outliers (annulations, et délais >2 ans)\n",
    "df_filtre_delai_ouverture = df_filtre_delai_ouverture[(df_filtre_delai_ouverture[\"annee_autorisation\"] >= 2015) & (df_filtre_delai_ouverture[\"annee_autorisation\"] <= 2019)]\n",
    "df_filtre_delai_ouverture = df_filtre_delai_ouverture[df_filtre_delai_ouverture[\"delai_ouverture_chantier\"] <= 600]\n",
    "df_filtre_delai_ouverture = df_filtre_delai_ouverture[df_filtre_delai_ouverture[\"ETAT_DAU\"] != 4] #on retire les annulations\n",
    "\n",
    "p95 = df_filtre_delai_ouverture[\"delai_ouverture_chantier\"].quantile(0.95)\n",
    "p99 = df_filtre_delai_ouverture[\"delai_ouverture_chantier\"].quantile(0.99)\n",
    "\n",
    "print(f\"Seuil 95e percentile : {p95:.1f} jours\")\n",
    "print(f\"Seuil 99e percentile : {p99:.1f} jours\")\n",
    "\n",
    "# Nettoyage et conversion des types sur l'échantillon\n",
    "df_filtre_delai_ouverture = df_filtre_delai_ouverture.drop(columns=vars_inutiles_delai_ouverture, errors=\"ignore\")\n",
    "\n",
    "# Suppression des lignes sans variable cible\n",
    "df_model = df_filtre_delai_ouverture.dropna(subset=[\"delai_ouverture_chantier\"])\n",
    "\n",
    "# On définit X et y\n",
    "y = df_model[\"delai_ouverture_chantier\"]\n",
    "X = df_model.drop(columns=[\"delai_ouverture_chantier\"])\n",
    "\n",
    "# Colonnes numériques\n",
    "num_cols = X.select_dtypes(\n",
    "    include=[\"float\", \"int\", \"bool\"]\n",
    ").columns.tolist()\n",
    "\n",
    "# Colonnes catégorielles\n",
    "cat_cols = X.select_dtypes(\n",
    "    include=[\"string\"]\n",
    ").columns.tolist()\n",
    "\n",
    "# On gère les NAs des variables explicatives\n",
    "cols_utiles = num_cols + cat_cols\n",
    "n_before_na = len(X)\n",
    "# Suppression des lignes avec NA sur les variables explicatives\n",
    "X = X.dropna(subset=cols_utiles)\n",
    "y = y.loc[X.index] \n",
    "\n",
    "n_after_na = len(X)\n",
    "drop_rate = 100 * (1 - n_after_na / n_before_na)\n",
    "\n",
    "print(f\"[INFO] Observations après filtrage NA : {n_after_na:,}\")\n",
    "print(f\"[INFO] Taux de suppression des observations : {drop_rate:.2f}%\")\n",
    "\n",
    "# B. Pipeline de pré-traitement et modèle LASSO\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Mise à l'échelle des variables numériques (StandardScaler)\n",
    "        (\"num\", StandardScaler(), num_cols), \n",
    "        # Encodage One-Hot des variables catégorielles (handle_unknown='ignore' pour les valeurs futures non vues)\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\n",
    "    ],\n",
    "    remainder='drop' # On jette ce qui reste\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f32e43d",
   "metadata": {},
   "source": [
    "# 5. Régression LASSO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf8587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.1, max_iter=10000)\n",
    "\n",
    "lasso_pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", lasso)\n",
    "])\n",
    "\n",
    "lasso_pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = lasso_pipeline.predict(X_train)\n",
    "y_test_pred = lasso_pipeline.predict(X_test)\n",
    "\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"RMSE train : {train_rmse:.2f}\")\n",
    "print(f\"RMSE test  : {test_rmse:.2f}\")\n",
    "print(f\"MAE train  : {train_mae:.2f}\")\n",
    "print(f\"MAE test   : {test_mae:.2f}\")\n",
    "print(f\"R² train   : {train_r2:.3f}\")\n",
    "print(f\"R² test    : {test_r2:.3f}\")\n",
    "\n",
    "print('Classification accuracy on test is: {}'.format(lasso_pipeline.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "# Nombre de coefficients non nuls\n",
    "coefs = lasso_pipeline.named_steps[\"model\"].coef_\n",
    "nb_nonzero = np.sum(coefs != 0)\n",
    "print(\"Nombre de coefficients non nuls :\", nb_nonzero)\n",
    "\n",
    "# Liste des coefficients non nuls \n",
    "feature_names = list(num_cols) + list(cat_cols)\n",
    "coefs = lasso_pipeline.named_steps[\"model\"].coef_\n",
    "nonzero_idx = coefs != 0\n",
    "selected_features = [(name, coef) for name, coef in zip(feature_names, coefs) if coef != 0]\n",
    "for name, coef in selected_features:\n",
    "    print(f\"{name}: {coef:.4f}\")\n",
    "\n",
    "## Essayons une version en log \n",
    "y_log = np.log1p(y)\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(\n",
    "    X, y_log,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lasso_log = Lasso(alpha=0.1, max_iter=10_000)\n",
    "\n",
    "lasso_log_pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", lasso_log)\n",
    "])\n",
    "lasso_log_pipeline.fit(X_train2, y_train2)\n",
    "\n",
    "y_train_log_pred = lasso_log_pipeline.predict(X_train2)\n",
    "y_test_log_pred = lasso_log_pipeline.predict(X_test2)\n",
    "\n",
    "train_r2_log = r2_score(y_train2, y_train_log_pred)\n",
    "test_r2_log = r2_score(y_test2, y_test_log_pred)\n",
    "\n",
    "print(\"\\n[LASSO – log(durée + 1)]\")\n",
    "print(f\"R² train   : {train_r2_log:.3f}\")\n",
    "print(f\"R² test    : {test_r2_log:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b2e677",
   "metadata": {},
   "source": [
    "# 6. Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a766c929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", RandomForestRegressor(\n",
    "        max_depth=12, \n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=4,\n",
    "        n_estimators=200,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    ))\n",
    "]) #On peut modifier la profondeur peut être pour améliorer les résultats \n",
    "\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_rf = rf_pipeline.predict(X_train)\n",
    "y_test_pred_rf  = rf_pipeline.predict(X_test)\n",
    "\n",
    "train_rmse_rf = np.sqrt(mean_squared_error(y_train, y_train_pred_rf))\n",
    "test_rmse_rf  = np.sqrt(mean_squared_error(y_test, y_test_pred_rf))\n",
    "\n",
    "train_mae_rf = mean_absolute_error(y_train, y_train_pred_rf)\n",
    "test_mae_rf  = mean_absolute_error(y_test, y_test_pred_rf)\n",
    "\n",
    "train_r2_rf = r2_score(y_train, y_train_pred_rf)\n",
    "test_r2_rf  = r2_score(y_test, y_test_pred_rf)\n",
    "\n",
    "print(f\"RMSE train : {train_rmse_rf:.2f}\")\n",
    "print(f\"RMSE test  : {test_rmse_rf:.2f}\")\n",
    "print(f\"MAE train  : {train_mae_rf:.2f}\")\n",
    "print(f\"MAE test   : {test_mae_rf:.2f}\")\n",
    "print(f\"R² train   : {train_r2_rf:.3f}\")\n",
    "print(f\"R² test    : {test_r2_rf:.3f}\")\n",
    "\n",
    "print('Classification accuracy on test is: {}'.format(rf_pipeline.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef7565c",
   "metadata": {},
   "source": [
    "# 7. Réseau de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e695d2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp_pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", MLPRegressor(\n",
    "        hidden_layer_sizes=(32, 16),   \n",
    "        activation=\"relu\",\n",
    "        solver=\"adam\",\n",
    "        alpha=1e-4,\n",
    "        learning_rate=\"adaptive\",\n",
    "        learning_rate_init=0.001,\n",
    "        batch_size=256,               \n",
    "        max_iter=300,                  \n",
    "        early_stopping=True,\n",
    "        n_iter_no_change=15,\n",
    "        tol=1e-4,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "mlp_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Prédictions\n",
    "y_train_pred_mlp = mlp_pipeline.predict(X_train)\n",
    "y_test_pred_mlp  = mlp_pipeline.predict(X_test)\n",
    "\n",
    "# Métriques\n",
    "train_rmse_mlp = np.sqrt(mean_squared_error(y_train, y_train_pred_mlp))\n",
    "test_rmse_mlp  = np.sqrt(mean_squared_error(y_test, y_test_pred_mlp))\n",
    "\n",
    "train_mae_mlp = mean_absolute_error(y_train, y_train_pred_mlp)\n",
    "test_mae_mlp  = mean_absolute_error(y_test, y_test_pred_mlp)\n",
    "\n",
    "train_r2_mlp = r2_score(y_train, y_train_pred_mlp)\n",
    "test_r2_mlp  = r2_score(y_test, y_test_pred_mlp)\n",
    "\n",
    "print(f\"[MLP] RMSE train : {train_rmse_mlp:.2f}\")\n",
    "print(f\"[MLP] RMSE test  : {test_rmse_mlp:.2f}\")\n",
    "print(f\"[MLP] MAE train  : {train_mae_mlp:.2f}\")\n",
    "print(f\"[MLP] MAE test   : {test_mae_mlp:.2f}\")\n",
    "print(f\"[MLP] R² train   : {train_r2_mlp:.3f}\")\n",
    "print(f\"[MLP] R² test    : {test_r2_mlp:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb4664b",
   "metadata": {},
   "source": [
    "# 8. SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174ec20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "svr_pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", LinearSVR(C=1.0, epsilon=1.0, random_state=0))\n",
    "])\n",
    "\n",
    "svr_pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_svr = svr_pipeline.predict(X_train)\n",
    "y_test_pred_svr  = svr_pipeline.predict(X_test)\n",
    "\n",
    "train_rmse_svr = np.sqrt(mean_squared_error(y_train, y_train_pred_svr))\n",
    "test_rmse_svr  = np.sqrt(mean_squared_error(y_test, y_test_pred_svr))\n",
    "\n",
    "train_mae_svr = mean_absolute_error(y_train, y_train_pred_svr)\n",
    "test_mae_svr  = mean_absolute_error(y_test, y_test_pred_svr)\n",
    "\n",
    "train_r2_svr = r2_score(y_train, y_train_pred_svr)\n",
    "test_r2_svr  = r2_score(y_test, y_test_pred_svr)\n",
    "\n",
    "print(f\"RMSE train : {train_rmse_svr:.2f}\")\n",
    "print(f\"RMSE test  : {test_rmse_svr:.2f}\")\n",
    "print(f\"MAE train  : {train_mae_svr:.2f}\")\n",
    "print(f\"MAE test   : {test_mae_svr:.2f}\")\n",
    "print(f\"R² train   : {train_r2_svr:.3f}\")\n",
    "print(f\"R² test    : {test_r2_svr:.3f}\")\n",
    "\n",
    "#print('Returned hyperparameter: {}'.format(svr_pipeline.best_params_))\n",
    "#print('Best classification accuracy in train is: {}'.format(svr_pipeline.best_score_))\n",
    "print('Classification accuracy on test is: {}'.format(svr_pipeline.score(X_test, y_test)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
